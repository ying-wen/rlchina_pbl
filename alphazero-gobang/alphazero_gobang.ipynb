{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用及第的1v1五子棋环境接口gamecore制作了一份alphazero tutorial，希望能够帮助大家理解alphago系列，主要分为以下几个部分：\n",
    "- 强化学习的网络结构及Loss函数\n",
    "- 蒙特卡洛树搜索和强化学习的结合\n",
    "- 训练过程，环境接口调用\n",
    "\n",
    "具体环境说明大家可参考链接：\n",
    "\n",
    "http://jidiai.cn/env_detail?envid=3\n",
    "\n",
    "https://github.com/jidiai/Competition_Gobang\n",
    "\n",
    "本tutorial参考了github链接：https://github.com/junxiaosong/AlphaZero_Gomoku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们介绍第一部分：强化学习的网络结构及Loss函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，引入必要的库函数和一些可能用到的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import copy\n",
    "from operator import itemgetter\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "def set_learning_rate(optimizer, lr):\n",
    "    \"\"\"Sets the learning rate to the given value\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def softmax(x):\n",
    "    probs = np.exp(x - np.max(x))\n",
    "    probs /= np.sum(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在及第的环境中，state是一个width * height的棋盘，棋盘元素有三种情况：\n",
    "- 0：该处未落子\n",
    "- 1：该处落下黑子（player1）\n",
    "- 2：该处落下白子（player2）\n",
    "\n",
    "在AlphaGo Zero中，一共使用了17个$19 \\times 19$的二值特征平面来描述当前局面，其中前16个平面描述了最近8步对应的双方player的棋子位置，最后一个平面描述当前player对应的棋子颜色，其实也就是先后手。在我们的实现中，对局面的描述进行了极大的简化，我们将其转化成2个width*height的棋盘，分别记录黑子的位置和白子的位置，大家也可以根据自己的认知和喜好进行修改/添加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_state(game):\n",
    "    square_state = np.zeros((2, game.board_width, game.board_height))\n",
    "    current_player = game.chess_player\n",
    "    for i in range(game.board_height):\n",
    "        for j in range(game.board_width):\n",
    "            if (game.current_state[i][j][0] == 0): continue\n",
    "            else:\n",
    "                if (game.current_state[i][j][0] == current_player):\n",
    "                    square_state[0][i][j] = 1\n",
    "                else:\n",
    "                    square_state[1][i][j] = 1\n",
    "    return square_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是具体的网络部分，参考AlphaZero，结合了策略网络和价值网络\n",
    "\n",
    "$Loss = \\sum_{t} (v_{\\theta}(s_t) - z_t)^2 - \\pi_t log(p_\\theta(s_t))$\n",
    "\n",
    "由于我们需要进行额外的MCTS搜索，所以我们额外增加了policy_value_fn()函数，针对当前的game state给出对应的[action, action_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"policy-value network module\"\"\"\n",
    "    def __init__(self, board_width, board_height):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.board_width = board_width\n",
    "        self.board_height = board_height\n",
    "        # common layers\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        # action policy layers\n",
    "        self.act_conv1 = nn.Conv2d(128, 2, kernel_size=1)\n",
    "        self.act_fc1 = nn.Linear(2*board_width*board_height,\n",
    "                                 board_width*board_height)\n",
    "        # state value layers\n",
    "        self.val_conv1 = nn.Conv2d(128, 2, kernel_size=1)\n",
    "        self.val_fc1 = nn.Linear(2*board_width*board_height, 64)\n",
    "        self.val_fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state_input):\n",
    "        # common layers\n",
    "        x = F.relu(self.conv1(state_input))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # action policy layers\n",
    "        x_act = F.relu(self.act_conv1(x))\n",
    "        x_act = x_act.view(-1, 2*self.board_width*self.board_height)\n",
    "        x_act = F.log_softmax(self.act_fc1(x_act))\n",
    "\n",
    "        # state value layers\n",
    "        x_val = F.relu(self.val_conv1(x))\n",
    "        x_val = x_val.view(-1, 2*self.board_width*self.board_height)\n",
    "        x_val = F.relu(self.val_fc1(x_val))\n",
    "        x_val = torch.tanh(self.val_fc2(x_val))\n",
    "        return x_act, x_val\n",
    "\n",
    "\n",
    "class PolicyValueNet():\n",
    "    \"\"\"policy-value network \"\"\"\n",
    "    def __init__(self, board_width, board_height,\n",
    "                 model_file=None, use_gpu=False):\n",
    "        self.use_gpu = use_gpu\n",
    "        self.board_width = board_width\n",
    "        self.board_height = board_height\n",
    "        self.l2_const = 1e-4  # coef of l2 penalty\n",
    "        # the policy value net module\n",
    "        if self.use_gpu:\n",
    "            self.policy_value_net = Net(board_width, board_height).cuda()\n",
    "        else:\n",
    "            self.policy_value_net = Net(board_width, board_height)\n",
    "        self.optimizer = optim.Adam(self.policy_value_net.parameters(),\n",
    "                                    weight_decay=self.l2_const)\n",
    "\n",
    "        if model_file:\n",
    "            net_params = torch.load(model_file)\n",
    "            self.policy_value_net.load_state_dict(net_params)\n",
    "\n",
    "    def policy_value(self, state_batch):\n",
    "        \"\"\"\n",
    "        input: a batch of states\n",
    "        output: a batch of action probabilities and state values\n",
    "        \"\"\"\n",
    "        if self.use_gpu:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch).cuda())\n",
    "            log_act_probs, value = self.policy_value_net(state_batch)\n",
    "            act_probs = np.exp(log_act_probs.data.cpu().numpy())\n",
    "            return act_probs, value.data.cpu().numpy()\n",
    "        else:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch))\n",
    "            log_act_probs, value = self.policy_value_net(state_batch)\n",
    "            act_probs = np.exp(log_act_probs.data.numpy())\n",
    "            return act_probs, value.data.numpy()\n",
    "\n",
    "    def policy_value_fn(self, game):\n",
    "        \"\"\"\n",
    "        input: game\n",
    "        output: a list of (action, probability) tuples for each available\n",
    "        action and the score of the game state\n",
    "        \"\"\"\n",
    "        legal_positions = game.all_grids\n",
    "        legal_positions = [item[0] * game.board_width + item[1] for item in legal_positions]\n",
    "        current_state = np.ascontiguousarray(get_current_state(game).reshape(\n",
    "                -1, 2, self.board_width, self.board_height))\n",
    "        if self.use_gpu:\n",
    "            log_act_probs, value = self.policy_value_net(\n",
    "                Variable(torch.from_numpy(current_state)).cuda().float())\n",
    "            act_probs = np.exp(log_act_probs.cpu().numpy().flatten())\n",
    "        else:\n",
    "            log_act_probs, value = self.policy_value_net(\n",
    "                Variable(torch.from_numpy(current_state)).float())\n",
    "            act_probs = np.exp(log_act_probs.data.numpy().flatten())\n",
    "        act_probs = zip(legal_positions, act_probs[legal_positions])\n",
    "        value = value.data[0][0]\n",
    "        return act_probs, value\n",
    "\n",
    "    def train_step(self, state_batch, mcts_probs, winner_batch, lr):\n",
    "        \"\"\"perform a training step\"\"\"\n",
    "        # wrap in Variable\n",
    "        if self.use_gpu:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch).cuda())\n",
    "            mcts_probs = Variable(torch.FloatTensor(mcts_probs).cuda())\n",
    "            winner_batch = Variable(torch.FloatTensor(winner_batch).cuda())\n",
    "        else:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch))\n",
    "            mcts_probs = Variable(torch.FloatTensor(mcts_probs))\n",
    "            winner_batch = Variable(torch.FloatTensor(winner_batch))\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        # set learning rate\n",
    "        set_learning_rate(self.optimizer, lr)\n",
    "\n",
    "        # forward\n",
    "        log_act_probs, value = self.policy_value_net(state_batch)\n",
    "        # define the loss = (z - v)^2 - pi^T * log(p) + c||theta||^2\n",
    "        # Note: the L2 penalty is incorporated in optimizer\n",
    "        value_loss = F.mse_loss(value.view(-1), winner_batch)\n",
    "        policy_loss = -torch.mean(torch.sum(mcts_probs*log_act_probs, 1))\n",
    "        loss = value_loss + policy_loss\n",
    "        # backward and optimize\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # calc policy entropy, for monitoring only\n",
    "        entropy = -torch.mean(\n",
    "                torch.sum(torch.exp(log_act_probs) * log_act_probs, 1)\n",
    "                )\n",
    "        return loss.item(), entropy.item()\n",
    "\n",
    "    def get_policy_param(self):\n",
    "        net_params = self.policy_value_net.state_dict()\n",
    "        return net_params\n",
    "\n",
    "    def save_model(self, model_file):\n",
    "        \"\"\" save model params to file \"\"\"\n",
    "        net_params = self.get_policy_param()  # get model params\n",
    "        torch.save(net_params, model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是蒙特卡洛树搜索部分，首先定义树节点，树节点需要实现四种操作：\n",
    "- 选择：在每个step，从当前state $s_t$选择动作$a_t$，其中\n",
    "\n",
    "    $a_t = argmax_a(Q(s_t,a)+u(s_t,a))$, Q为节点value值\n",
    "\n",
    "    $u = c_{puct}P(s,a) \\frac{\\sqrt{(\\sum_b N_r(s,b))}}{1+N_r(s,a)}$\n",
    "- 扩张：到达叶子节点时，可根据策略网络的输出对蒙特卡洛树进行扩充\n",
    "\n",
    "    {$N(s_L,a) = 0,W(s_L,a)=0, Q(s_L,a)=0, P(s_L,a)=p_\\sigma(a|s_L)$}\n",
    "- 评估：根据价值网络的输出和环境结果进行更新\n",
    "- 反向传播：更新节点Q值和visit数目N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    \"\"\"A node in the MCTS tree.\n",
    "\n",
    "    Each node keeps track of its own value Q, prior probability P, and\n",
    "    its visit-count-adjusted prior score u.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parent, prior_p):\n",
    "        self._parent = parent\n",
    "        self._children = {}  # a map from action to TreeNode\n",
    "        self._n_visits = 0\n",
    "        self._Q = 0\n",
    "        self._u = 0\n",
    "        self._P = prior_p\n",
    "\n",
    "    def expand(self, action_priors):\n",
    "        \"\"\"Expand tree by creating new children.\n",
    "        action_priors: a list of tuples of actions and their prior probability\n",
    "            according to the policy function.\n",
    "        \"\"\"\n",
    "        for action, prob in action_priors:\n",
    "            if action not in self._children:\n",
    "                self._children[action] = TreeNode(self, prob)\n",
    "\n",
    "    def select(self, c_puct):\n",
    "        \"\"\"Select action among children that gives maximum action value Q\n",
    "        plus bonus u(P).\n",
    "        Return: A tuple of (action, next_node)\n",
    "        \"\"\"\n",
    "        return max(self._children.items(),\n",
    "                   key=lambda act_node: act_node[1].get_value(c_puct))\n",
    "\n",
    "    def update(self, leaf_value):\n",
    "        \"\"\"Update node values from leaf evaluation.\n",
    "        leaf_value: the value of subtree evaluation from the current player's\n",
    "            perspective.\n",
    "        \"\"\"\n",
    "        # Count visit.\n",
    "        self._n_visits += 1\n",
    "        # Update Q, a running average of values for all visits.\n",
    "        self._Q += 1.0*(leaf_value - self._Q) / self._n_visits\n",
    "\n",
    "    def update_recursive(self, leaf_value):\n",
    "        \"\"\"Like a call to update(), but applied recursively for all ancestors.\n",
    "        \"\"\"\n",
    "        # If it is not root, this node's parent should be updated first.\n",
    "        if self._parent:\n",
    "            self._parent.update_recursive(-leaf_value)\n",
    "        self.update(leaf_value)\n",
    "\n",
    "    def get_value(self, c_puct):\n",
    "        \"\"\"Calculate and return the value for this node.\n",
    "        It is a combination of leaf evaluations Q, and this node's prior\n",
    "        adjusted for its visit count, u.\n",
    "        c_puct: a number in (0, inf) controlling the relative impact of\n",
    "            value Q, and prior probability P, on this node's score.\n",
    "        \"\"\"\n",
    "        self._u = (c_puct * self._P *\n",
    "                   np.sqrt(self._parent._n_visits) / (1 + self._n_visits))\n",
    "        return self._Q + self._u\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"Check if leaf node (i.e. no nodes below this have been expanded).\"\"\"\n",
    "        return self._children == {}\n",
    "\n",
    "    def is_root(self):\n",
    "        return self._parent is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS(object):\n",
    "    \"\"\"An implementation of Monte Carlo Tree Search.\"\"\"\n",
    "\n",
    "    def __init__(self, policy_value_fn, c_puct=5, n_playout=10000):\n",
    "        \"\"\"\n",
    "        policy_value_fn: a function that takes in a game state and outputs\n",
    "            a list of (action, probability) tuples and also a score in [-1, 1]\n",
    "            (i.e. the expected value of the end game score from the current\n",
    "            player's perspective) for the current player.\n",
    "        c_puct: a number in (0, inf) that controls how quickly exploration\n",
    "            converges to the maximum-value policy. A higher value means\n",
    "            relying on the prior more.\n",
    "        \"\"\"\n",
    "        self._root = TreeNode(None, 1.0)\n",
    "        self._policy = policy_value_fn\n",
    "        self._c_puct = c_puct\n",
    "        self._n_playout = n_playout\n",
    "\n",
    "    def _playout(self, game):\n",
    "        \"\"\"Run a single playout from the root to the leaf, getting a value at\n",
    "        the leaf and propagating it back through its parents.\n",
    "        State is modified in-place, so a copy must be provided.\n",
    "        \"\"\"\n",
    "        node = self._root\n",
    "        while(1):\n",
    "            if node.is_leaf():\n",
    "                break\n",
    "            # Greedily select next move\n",
    "            action, node = node.select(self._c_puct) # action = move, for example, 52\n",
    "            joint_action = game.encode(action//game.board_width, action % game.board_width) # row, column\n",
    "            game.get_next_state(joint_action)\n",
    "        \n",
    "        # Evaluate the leaf using a network which outputs a list of\n",
    "        # (action, probability) tuples p and also a score v in [-1, 1]\n",
    "        # for the current player.\n",
    "        action_probs, leaf_value = self._policy(game)\n",
    "        # check for end of game\n",
    "        end = game.is_terminal()\n",
    "        winner = game.check_win()\n",
    "        if not end:\n",
    "            node.expand(action_probs)\n",
    "        else:\n",
    "            # for end state, return the \"true\" leaf_value\n",
    "            if winner == 0: #tie\n",
    "                leaf_value = 0.0\n",
    "            else:\n",
    "                leaf_value = (\n",
    "                    1.0 if winner == game.chess_player else -1.0\n",
    "                )\n",
    "        \n",
    "        # Update value and visit count of nodes in this traversal\n",
    "        node.update_recursive(-leaf_value)\n",
    "        \n",
    "\n",
    "    def get_move_probs(self, state, temp=1e-3):\n",
    "        \"\"\"Run all playouts sequentially and return the available actions and\n",
    "        their corresponding probabilities.\n",
    "        state: the current game state\n",
    "        temp: temperature parameter in (0, 1] controls the level of exploration\n",
    "        \"\"\"\n",
    "        for n in range(self._n_playout):\n",
    "            state_copy = copy.deepcopy(state)\n",
    "            self._playout(state_copy)\n",
    "\n",
    "        # calc the move probabilities based on visit counts at the root node\n",
    "        act_visits = [(act, node._n_visits)\n",
    "                      for act, node in self._root._children.items()]\n",
    "        acts, visits = zip(*act_visits)\n",
    "        act_probs = softmax(1.0/temp * np.log(np.array(visits) + 1e-10))\n",
    "\n",
    "        return acts, act_probs\n",
    "\n",
    "    def update_with_move(self, last_move):\n",
    "        \"\"\"Step forward in the tree, keeping everything we already know\n",
    "        about the subtree.\n",
    "        \"\"\"\n",
    "        if last_move in self._root._children:\n",
    "            self._root = self._root._children[last_move]\n",
    "            self._root._parent = None\n",
    "        else:\n",
    "            self._root = TreeNode(None, 1.0)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"MCTS\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来对MCTS进行封装，为了鼓励探索，alphazero中，每一个self-play对局的前30步，action是根据正比于MCTS根节点处每个分支的访问次数的概率采样得到的，之后的exploration则是通过直接加上Dirichlet noise的方式实现的($P(s,a)=(1-\\epsilon)P_a + \\epsilon \\eta_a$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSPlayer(object):\n",
    "    \"\"\"AI player based on MCTS\"\"\"\n",
    "\n",
    "    def __init__(self, policy_value_function,\n",
    "                 c_puct=5, n_playout=2000, is_selfplay=0):\n",
    "        self.mcts = MCTS(policy_value_function, c_puct, n_playout)\n",
    "        self._is_selfplay = is_selfplay\n",
    "\n",
    "    def set_player_ind(self, p):\n",
    "        self.player = p\n",
    "\n",
    "    def reset_player(self):\n",
    "        self.mcts.update_with_move(-1)\n",
    "\n",
    "    def get_action(self, game, temp=1e-3, return_prob=0):\n",
    "        sensible_moves = game.all_grids\n",
    "        # the pi vector returned by MCTS as in the alphaGo Zero paper\n",
    "        move_probs = np.zeros(game.board_width * game.board_height)\n",
    "        if len(sensible_moves) > 0:\n",
    "            acts, probs = self.mcts.get_move_probs(game, temp)\n",
    "            move_probs[list(acts)] = probs\n",
    "            if self._is_selfplay:\n",
    "                # add Dirichlet Noise for exploration (needed for self-play training)\n",
    "                move = np.random.choice(acts, p=0.75*probs + 0.25*np.random.dirichlet(0.3*np.ones(len(probs))))\n",
    "                # update the root node and reuse the search tree\n",
    "                self.mcts.update_with_move(move)\n",
    "            else:\n",
    "                # with the default temp=1e-3, it is almost equivalent to choosing the move with the highest prob\n",
    "                move = np.random.choice(acts, p=probs)\n",
    "                # reset the root node\n",
    "                self.mcts.update_with_move(-1)\n",
    "            \n",
    "            if return_prob:\n",
    "                return move, move_probs\n",
    "            else:\n",
    "                return move\n",
    "        else:\n",
    "            print(\"WARNING: the board is full\")\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"MCTS {}\".format(self.player)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们提供了一个不含强化部分的MCTS用于对比和参考，也可用于策略评估部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_policy_fn(game):\n",
    "    \"\"\"a coarse, fast version of policy_fn used in the rollout phase.\"\"\"\n",
    "    # rollout randomly\n",
    "    action_probs = np.random.rand(len(game.all_grids))\n",
    "    return zip(game.all_grids, action_probs)\n",
    "\n",
    "\n",
    "def policy_value_fn(game):\n",
    "    \"\"\"a function that takes in a state and outputs a list of (action, probability)\n",
    "    tuples and a score for the state\"\"\"\n",
    "    # return uniform probabilities and 0 score for pure MCTS\n",
    "    action_probs = np.ones(len(game.all_grids))/len(game.all_grids)\n",
    "    return zip(game.all_grids, action_probs), 0\n",
    "\n",
    "\n",
    "class TreeNode_Pure(object):\n",
    "    \"\"\"A node in the MCTS tree. Each node keeps track of its own value Q,\n",
    "    prior probability P, and its visit-count-adjusted prior score u.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parent, prior_p):\n",
    "        self._parent = parent\n",
    "        self._children = {}  # a map from action to TreeNode\n",
    "        self._n_visits = 0\n",
    "        self._Q = 0\n",
    "        self._u = 0\n",
    "        self._P = prior_p\n",
    "\n",
    "    def expand(self, action_priors):\n",
    "        \"\"\"Expand tree by creating new children.\n",
    "        action_priors: a list of tuples of actions and their prior probability\n",
    "            according to the policy function.\n",
    "        \"\"\"\n",
    "        for action, prob in action_priors:\n",
    "            if action not in self._children:\n",
    "                self._children[action] = TreeNode_Pure(self, prob)\n",
    "\n",
    "    def select(self, c_puct):\n",
    "        \"\"\"Select action among children that gives maximum action value Q\n",
    "        plus bonus u(P).\n",
    "        Return: A tuple of (action, next_node)\n",
    "        \"\"\"\n",
    "        return max(self._children.items(),\n",
    "                   key=lambda act_node: act_node[1].get_value(c_puct))\n",
    "\n",
    "    def update(self, leaf_value):\n",
    "        \"\"\"Update node values from leaf evaluation.\n",
    "        leaf_value: the value of subtree evaluation from the current player's\n",
    "            perspective.\n",
    "        \"\"\"\n",
    "        # Count visit.\n",
    "        self._n_visits += 1\n",
    "        # Update Q, a running average of values for all visits.\n",
    "        self._Q += 1.0*(leaf_value - self._Q) / self._n_visits\n",
    "\n",
    "    def update_recursive(self, leaf_value):\n",
    "        \"\"\"Like a call to update(), but applied recursively for all ancestors.\n",
    "        \"\"\"\n",
    "        # If it is not root, this node's parent should be updated first.\n",
    "        if self._parent:\n",
    "            self._parent.update_recursive(-leaf_value)\n",
    "        self.update(leaf_value)\n",
    "\n",
    "    def get_value(self, c_puct):\n",
    "        \"\"\"Calculate and return the value for this node.\n",
    "        It is a combination of leaf evaluations Q, and this node's prior\n",
    "        adjusted for its visit count, u.\n",
    "        c_puct: a number in (0, inf) controlling the relative impact of\n",
    "            value Q, and prior probability P, on this node's score.\n",
    "        \"\"\"\n",
    "        self._u = (c_puct * self._P *\n",
    "                   np.sqrt(self._parent._n_visits) / (1 + self._n_visits))\n",
    "        return self._Q + self._u\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"Check if leaf node (i.e. no nodes below this have been expanded).\n",
    "        \"\"\"\n",
    "        return self._children == {}\n",
    "\n",
    "    def is_root(self):\n",
    "        return self._parent is None\n",
    "\n",
    "\n",
    "class MCTS_Pure(object):\n",
    "    \"\"\"A simple implementation of Monte Carlo Tree Search.\"\"\"\n",
    "\n",
    "    def __init__(self, policy_value_fn, c_puct=5, n_playout=10000):\n",
    "        \"\"\"\n",
    "        policy_value_fn: a function that takes in a board state and outputs\n",
    "            a list of (action, probability) tuples and also a score in [-1, 1]\n",
    "            (i.e. the expected value of the end game score from the current\n",
    "            player's perspective) for the current player.\n",
    "        c_puct: a number in (0, inf) that controls how quickly exploration\n",
    "            converges to the maximum-value policy. A higher value means\n",
    "            relying on the prior more.\n",
    "        \"\"\"\n",
    "        self._root = TreeNode_Pure(None, 1.0)\n",
    "        self._policy = policy_value_fn\n",
    "        self._c_puct = c_puct\n",
    "        self._n_playout = n_playout\n",
    "\n",
    "    def _playout(self, game):\n",
    "        \"\"\"Run a single playout from the root to the leaf, getting a value at\n",
    "        the leaf and propagating it back through its parents.\n",
    "        State is modified in-place, so a copy must be provided.\n",
    "        \"\"\"\n",
    "        node = self._root\n",
    "        while(1):\n",
    "            if node.is_leaf():\n",
    "                break\n",
    "            # Greedily select next move\n",
    "            action, node = node.select(self._c_puct)\n",
    "            joint_action = game.encode(action[0], action[1])\n",
    "            game.get_next_state(joint_action)\n",
    "        \n",
    "        action_probs, _ = self._policy(game)\n",
    "        # check for end of game\n",
    "        end = game.is_terminal()\n",
    "        winner = game.check_win()\n",
    "        if not end:\n",
    "            node.expand(action_probs)\n",
    "        # Evaluate the leaf node by random rollout\n",
    "        leaf_value = self._evaluate_rollout(game)\n",
    "        # Update value and visit count of nodes in this terminal\n",
    "        node.update_recursive(-leaf_value)\n",
    "\n",
    "\n",
    "    def _evaluate_rollout(self, game, limit=1000):\n",
    "        \"\"\"Use the rollout policy to play until the end of the game,\n",
    "        returning +1 if the current player wins, -1 if the opponent wins,\n",
    "        and 0 if it is a tie.\n",
    "        \"\"\"\n",
    "        player = game.chess_player\n",
    "        for i in range(limit):\n",
    "            end = game.is_terminal()\n",
    "            winner = game.check_win()\n",
    "            if end:\n",
    "                break\n",
    "            action_probs = rollout_policy_fn(game)\n",
    "            max_action = max(action_probs, key=itemgetter(1))[0]\n",
    "            joint_action = game.encode(max_action[0], max_action[1])\n",
    "            game.get_next_state(joint_action)\n",
    "        else:\n",
    "            # If no break from the loop, issue a warning.\n",
    "            print(\"WARNING: rollout reached move limit\")\n",
    "        if winner == 0:  # tie\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 if winner == player else -1\n",
    "\n",
    "    def get_move(self, state):\n",
    "        \"\"\"Runs all playouts sequentially and returns the most visited action.\n",
    "        state: the current game state\n",
    "\n",
    "        Return: the selected action\n",
    "        \"\"\"\n",
    "        for n in range(self._n_playout):\n",
    "            state_copy = copy.deepcopy(state)\n",
    "            self._playout(state_copy)\n",
    "        return max(self._root._children.items(),\n",
    "                   key=lambda act_node: act_node[1]._n_visits)[0]\n",
    "\n",
    "    def update_with_move(self, last_move):\n",
    "        \"\"\"Step forward in the tree, keeping everything we already know\n",
    "        about the subtree.\n",
    "        \"\"\"\n",
    "        if last_move in self._root._children:\n",
    "            self._root = self._root._children[last_move]\n",
    "            self._root._parent = None\n",
    "        else:\n",
    "            self._root = TreeNode(None, 1.0)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"MCTS\"\n",
    "\n",
    "\n",
    "class MCTS_Pure_Player(object):\n",
    "    \"\"\"AI player based on MCTS\"\"\"\n",
    "    def __init__(self, c_puct=5, n_playout=2000):\n",
    "        self.mcts = MCTS_Pure(policy_value_fn, c_puct, n_playout)\n",
    "\n",
    "    def set_player_ind(self, p):\n",
    "        self.player = p\n",
    "\n",
    "    def reset_player(self):\n",
    "        self.mcts.update_with_move(-1)\n",
    "\n",
    "    def get_action(self, game):\n",
    "        sensible_moves = game.all_grids\n",
    "        if len(sensible_moves) > 0:\n",
    "            move = self.mcts.get_move(game)\n",
    "            self.mcts.update_with_move(-1)\n",
    "            return move\n",
    "        else:\n",
    "            print(\"WARNING: the board is full\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"MCTS {}\".format(self.player)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后是激动人心的训练时刻，在做训练前，我们需要做一些准备函数，例如game_play()和game_self_play()，在此我们要调用及第的接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"./gobang\")\n",
    "from env.chooseenv import make\n",
    "from utils.get_logger import get_logger\n",
    "from env.obs_interfaces.observation import obs_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们提供了两个play函数，start_play主要用于评估阶段，MCTS+RL与MCTS_Pure之间进行对战，start_self_play主要用于训练阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_play(game, player1, player2, start_player=1, is_shown=0):\n",
    "    \"\"\"start a game between two players\"\"\"\n",
    "    game.reset()\n",
    "    if (start_player == 0):\n",
    "        players = {1: player1, 2: player2}\n",
    "    else:\n",
    "        players = {1: player2, 2: player1}\n",
    "    while True:\n",
    "        current_player = game.chess_player\n",
    "        player_in_turn = players[current_player]\n",
    "        move = player_in_turn.get_action(game)\n",
    "        # two players have different move type, so we use different ways to handle them\n",
    "        if player_in_turn == player1:\n",
    "            joint_action = game.encode(move//game.board_width, move % game.board_width)\n",
    "        else:\n",
    "            joint_action = game.encode(move[0], move[1])\n",
    "        game.get_next_state(joint_action)\n",
    "        end = game.is_terminal()\n",
    "        winner = game.check_win()\n",
    "        if end:\n",
    "            if is_shown:\n",
    "                if winner != 0:\n",
    "                    print(\"Game end. Winner is\", players[winner])\n",
    "                else:\n",
    "                    print(\"Game end. Tie\")\n",
    "            return winner\n",
    "\n",
    "def start_self_play(game, player, is_shown=0, temp=1e-3):\n",
    "    game.reset()\n",
    "    states, mcts_probs, current_players = [],[],[]\n",
    "    while True:\n",
    "        move, move_probs = player.get_action(game, temp=temp, return_prob=1)\n",
    "        # store the data\n",
    "        states.append(get_current_state(game))\n",
    "        mcts_probs.append(move_probs)\n",
    "        current_players.append(game.chess_player)\n",
    "        # perform a move\n",
    "        joint_action = game.encode(move//game.board_width, move % game.board_width)\n",
    "        game.get_next_state(joint_action)\n",
    "        end = game.is_terminal()\n",
    "        winner = game.check_win()\n",
    "        if end:\n",
    "            # winner from the perspective of the current player of each state\n",
    "            winner_z = np.zeros(len(current_players))\n",
    "            if winner != 0:\n",
    "                winner_z[np.array(current_players) == winner] = 1.0\n",
    "                winner_z[np.array(current_players) != winner] = -1.0\n",
    "            # reset MCTS root node\n",
    "            player.reset_player()\n",
    "            if is_shown:\n",
    "                if winner != 0:\n",
    "                    print(\"Game end. Winner is player:\", winner)\n",
    "                else:\n",
    "                    print(\"Game end. Tie\")\n",
    "            return winner, zip(states, mcts_probs, winner_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是最终的训练阶段，采取简单的并行方式进行加速，使用get_equi_data对数据进行增强，加快训练速度（可注释），此外每50局自博弈后都会和Pure MCTS对打20局（局数可根据需要调整），并维持一个打Pure MCTS胜率最高的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainPipeline():\n",
    "    def __init__(self, init_model=None):\n",
    "        # params of the board and the game\n",
    "        self.board_width = 8\n",
    "        self.board_height = 8\n",
    "        self.n_in_row = 5\n",
    "        self.env_type = \"gobang_1v1\"\n",
    "        self.game = make(self.env_type, seed=None)\n",
    "        \n",
    "        # training params\n",
    "        self.learn_rate = 2e-3\n",
    "        self.lr_multiplier = 1.0  # adaptively adjust the learning rate based on KL\n",
    "        self.temp = 1.0  # the temperature param\n",
    "        self.n_playout = 400  # num of simulations for each move\n",
    "        self.c_puct = 5\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 512  # mini-batch size for training\n",
    "        self.data_buffer = deque(maxlen=self.buffer_size)\n",
    "        self.play_batch_size = 1\n",
    "        self.epochs = 5  # num of train_steps for each update\n",
    "        self.kl_targ = 0.02\n",
    "        self.check_freq = 50\n",
    "        self.game_batch_num = 1500\n",
    "        self.best_win_ratio = 0.0\n",
    "        # num of simulations used for the pure mcts, which is used as\n",
    "        # the opponent to evaluate the trained policy\n",
    "        self.pure_mcts_playout_num = 1000\n",
    "        if init_model:\n",
    "            # start training from an initial policy-value net\n",
    "            self.policy_value_net = PolicyValueNet(self.board_width,\n",
    "                                                   self.board_height,\n",
    "                                                   model_file=init_model)\n",
    "        else:\n",
    "            # start training from a new policy-value net\n",
    "            self.policy_value_net = PolicyValueNet(self.board_width,\n",
    "                                                   self.board_height)\n",
    "        self.mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
    "                                      c_puct=self.c_puct,\n",
    "                                      n_playout=self.n_playout,\n",
    "                                      is_selfplay=1)\n",
    "\n",
    "    def get_equi_data(self, play_data):\n",
    "        \"\"\"augment the data set by rotation and flipping\n",
    "        play_data: [(state, mcts_prob, winner_z), ..., ...]\n",
    "        \"\"\"\n",
    "        extend_data = []\n",
    "        for state, mcts_porb, winner in play_data:\n",
    "            for i in [1, 2, 3, 4]:\n",
    "                # rotate counterclockwise\n",
    "                equi_state = np.array([np.rot90(s, i) for s in state])\n",
    "                equi_mcts_prob = np.rot90(np.flipud(\n",
    "                    mcts_porb.reshape(self.board_height, self.board_width)), i)\n",
    "                extend_data.append((equi_state,\n",
    "                                    np.flipud(equi_mcts_prob).flatten(),\n",
    "                                    winner))\n",
    "                # flip horizontally\n",
    "                equi_state = np.array([np.fliplr(s) for s in equi_state])\n",
    "                equi_mcts_prob = np.fliplr(equi_mcts_prob)\n",
    "                extend_data.append((equi_state,\n",
    "                                    np.flipud(equi_mcts_prob).flatten(),\n",
    "                                    winner))\n",
    "        return extend_data\n",
    "\n",
    "    def collect_selfplay_data(self, n_games=1):\n",
    "        \"\"\"collect self-play data for training\"\"\"\n",
    "        for i in range(n_games):\n",
    "            winner, play_data = start_self_play(self.game, self.mcts_player,\n",
    "                                                          temp=self.temp)\n",
    "            play_data = list(play_data)[:]\n",
    "            self.episode_len = len(play_data)\n",
    "            # augment the data\n",
    "            play_data = self.get_equi_data(play_data)\n",
    "            self.data_buffer.extend(play_data)\n",
    "\n",
    "    def policy_update(self):\n",
    "        \"\"\"update the policy-value net\"\"\"\n",
    "        mini_batch = random.sample(self.data_buffer, self.batch_size)\n",
    "        state_batch = [data[0] for data in mini_batch]\n",
    "        mcts_probs_batch = [data[1] for data in mini_batch]\n",
    "        winner_batch = [data[2] for data in mini_batch]\n",
    "        old_probs, old_v = self.policy_value_net.policy_value(state_batch)\n",
    "        for i in range(self.epochs):\n",
    "            loss, entropy = self.policy_value_net.train_step(\n",
    "                    state_batch,\n",
    "                    mcts_probs_batch,\n",
    "                    winner_batch,\n",
    "                    self.learn_rate*self.lr_multiplier)\n",
    "            new_probs, new_v = self.policy_value_net.policy_value(state_batch)\n",
    "            kl = np.mean(np.sum(old_probs * (\n",
    "                    np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)),\n",
    "                    axis=1)\n",
    "            )\n",
    "            if kl > self.kl_targ * 4:  # early stopping if D_KL diverges badly\n",
    "                break\n",
    "        # adaptively adjust the learning rate\n",
    "        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:\n",
    "            self.lr_multiplier /= 1.5\n",
    "        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:\n",
    "            self.lr_multiplier *= 1.5\n",
    "\n",
    "        explained_var_old = (1 -\n",
    "                             np.var(np.array(winner_batch) - old_v.flatten()) /\n",
    "                             np.var(np.array(winner_batch)))\n",
    "        explained_var_new = (1 -\n",
    "                             np.var(np.array(winner_batch) - new_v.flatten()) /\n",
    "                             np.var(np.array(winner_batch)))\n",
    "        print((\"kl:{:.5f},\"\n",
    "               \"lr_multiplier:{:.3f},\"\n",
    "               \"loss:{},\"\n",
    "               \"entropy:{},\"\n",
    "               \"explained_var_old:{:.3f},\"\n",
    "               \"explained_var_new:{:.3f}\"\n",
    "               ).format(kl,\n",
    "                        self.lr_multiplier,\n",
    "                        loss,\n",
    "                        entropy,\n",
    "                        explained_var_old,\n",
    "                        explained_var_new))\n",
    "        return loss, entropy\n",
    "\n",
    "    def policy_evaluate(self, n_games=20):\n",
    "        \"\"\"\n",
    "        Evaluate the trained policy by playing against the pure MCTS player\n",
    "        Note: this is only for monitoring the progress of training\n",
    "        \"\"\"\n",
    "        current_mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
    "                                         c_puct=self.c_puct,\n",
    "                                         n_playout=self.n_playout)\n",
    "        pure_mcts_player = MCTS_Pure_Player(c_puct=5,\n",
    "                                     n_playout=self.pure_mcts_playout_num)\n",
    "        win_cnt = defaultdict(int)\n",
    "        for i in range(n_games):\n",
    "            winner = start_play(self.game, current_mcts_player,\n",
    "                                          pure_mcts_player,\n",
    "                                          start_player=i % 2,\n",
    "                                          is_shown=0)\n",
    "            win_cnt[winner] += 1\n",
    "        win_ratio = 1.0*(win_cnt[1] + 0.5*win_cnt[-1]) / n_games\n",
    "        print(\"num_playouts:{}, win: {}, lose: {}, tie:{}\".format(\n",
    "                self.pure_mcts_playout_num,\n",
    "                win_cnt[1], win_cnt[2], win_cnt[-1]))\n",
    "        return win_ratio\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"run the training pipeline\"\"\"\n",
    "        try:\n",
    "            for i in range(self.game_batch_num):\n",
    "                self.collect_selfplay_data(self.play_batch_size)\n",
    "                print(\"batch :{}, episode_len:{}\".format(\n",
    "                        i+1, self.episode_len))\n",
    "                if len(self.data_buffer) > self.batch_size:\n",
    "                    loss, entropy = self.policy_update()\n",
    "                # check the performance of the current model,\n",
    "                # and save the model params\n",
    "                if (i+1) % self.check_freq == 0:\n",
    "                    print(\"current self-play batch: {}\".format(i+1))\n",
    "                    win_ratio = self.policy_evaluate()\n",
    "                    self.policy_value_net.save_model('./current_policy.model')\n",
    "                    if win_ratio > self.best_win_ratio:\n",
    "                        print(\"New best policy!!!!!!!!\")\n",
    "                        self.best_win_ratio = win_ratio\n",
    "                        # update the best_policy\n",
    "                        self.policy_value_net.save_model('./best_policy.model')\n",
    "                        if (self.best_win_ratio == 1.0 and\n",
    "                                self.pure_mcts_playout_num < 5000):\n",
    "                            self.pure_mcts_playout_num += 1000\n",
    "                            self.best_win_ratio = 0.0\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\n\\rquit')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，开始你的训练吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/var/folders/8v/43mm4qxn1p1chwcx4r9lcfhm0000gn/T/ipykernel_84013/2045127855.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x_act = F.log_softmax(self.act_fc1(x_act))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :1, episode_len:23\n",
      "\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    training_pipeline = TrainPipeline()\n",
    "    training_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "64aa2f8c5ebe48bbebe3ae56eb4bb3101808594cbdf1a3a324eb84c2672c49d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
