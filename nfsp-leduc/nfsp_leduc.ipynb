{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leduc Hold'em 是一个简化版的德州扑克，游戏使用 6 张牌（红桃 J、Q、K，黑桃 J、Q、K），牌型大小比较中 对牌 > 单牌，K>Q>J，目标是赢得更多的筹码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "import rlcard\n",
    "from random_agent import RandomAgent\n",
    "from dqn_agent import DQNAgent\n",
    "from nfsp_agent import NFSPAgent\n",
    "from rlcard.utils import (\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    tournament,\n",
    "    reorganize,\n",
    "    Logger,\n",
    "    plot_curve,\n",
    ")\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    # Check whether gpu is available\n",
    "    device = get_device()\n",
    "        \n",
    "    # Seed numpy, torch, random\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # Make the environment with seed\n",
    "    env = rlcard.make(\n",
    "        args.env,\n",
    "        config={\n",
    "            'seed': args.seed,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Initialize the agent and use random agents as opponents\n",
    "    if args.algorithm == 'dqn':\n",
    "        \n",
    "        agent = DQNAgent(\n",
    "            num_actions=env.num_actions,\n",
    "            state_shape=env.state_shape[0],\n",
    "            mlp_layers=[64,64],\n",
    "            device=device,\n",
    "        )\n",
    "    elif args.algorithm == 'nfsp':\n",
    "        \n",
    "        agent = NFSPAgent(\n",
    "            num_actions=env.num_actions,\n",
    "            state_shape=env.state_shape[0],\n",
    "            hidden_layers_sizes=[64,64],\n",
    "            q_mlp_layers=[64,64],\n",
    "            device=device,\n",
    "        )\n",
    "    agents = [agent]\n",
    "    for _ in range(1, env.num_players):\n",
    "        agents.append(RandomAgent(num_actions=env.num_actions))\n",
    "        # agents.append(agent)\n",
    "    env.set_agents(agents)\n",
    "\n",
    "    # Start training\n",
    "    with Logger(args.log_dir) as logger:\n",
    "        for episode in range(args.num_episodes):\n",
    "\n",
    "            if args.algorithm == 'nfsp':\n",
    "                agents[0].sample_episode_policy()\n",
    "\n",
    "            # Generate data from the environment\n",
    "            trajectories, payoffs = env.run(is_training=True)\n",
    "\n",
    "            # Reorganaize the data to be state, action, reward, next_state, done\n",
    "            trajectories = reorganize(trajectories, payoffs)\n",
    "\n",
    "            # Feed transitions into agent memory, and train the agent\n",
    "            # Here, we assume that DQN always plays the first position\n",
    "            # and the other players play randomly (if any)\n",
    "            for ts in trajectories[0]:\n",
    "                agent.feed(ts)\n",
    "\n",
    "            # Evaluate the performance. Play with random agents.\n",
    "            if episode % args.evaluate_every == 0:\n",
    "                logger.log_performance(\n",
    "                    env.timestep,\n",
    "                    tournament(\n",
    "                        env,\n",
    "                        args.num_eval_games,\n",
    "                    )[0]\n",
    "                )\n",
    "\n",
    "        # Get the paths\n",
    "        csv_path, fig_path = logger.csv_path, logger.fig_path\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plot_curve(csv_path, fig_path, args.algorithm)\n",
    "\n",
    "    # Save model\n",
    "    save_path = os.path.join(args.log_dir, 'model.pth')\n",
    "    torch.save(agent, save_path)\n",
    "    print('Model saved in', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "import rlcard\n",
    "from random_agent import RandomAgent\n",
    "from dqn_agent import DQNAgent\n",
    "from nfsp_agent import NFSPAgent\n",
    "from rlcard.utils import (\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    tournament,\n",
    "    reorganize,\n",
    "    Logger,\n",
    "    plot_curve,\n",
    ")\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    # Check whether gpu is available\n",
    "    device = get_device()\n",
    "        \n",
    "    # Seed numpy, torch, random\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # Make the environment with seed\n",
    "    env = rlcard.make(\n",
    "        args.env,\n",
    "        config={\n",
    "            'seed': args.seed,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Initialize the agent and use random agents as opponents\n",
    "    if args.algorithm == 'dqn':\n",
    "        \n",
    "        agent = DQNAgent(\n",
    "            num_actions=env.num_actions,\n",
    "            state_shape=env.state_shape[0],\n",
    "            mlp_layers=[64,64],\n",
    "            device=device,\n",
    "        )\n",
    "    elif args.algorithm == 'nfsp':\n",
    "        \n",
    "        agent = NFSPAgent(\n",
    "            num_actions=env.num_actions,\n",
    "            state_shape=env.state_shape[0],\n",
    "            hidden_layers_sizes=[64,64],\n",
    "            q_mlp_layers=[64,64],\n",
    "            device=device,\n",
    "        )\n",
    "    agents = [agent]\n",
    "    for _ in range(1, env.num_players):\n",
    "        agents.append(RandomAgent(num_actions=env.num_actions))\n",
    "        # agents.append(agent)\n",
    "    env.set_agents(agents)\n",
    "\n",
    "    # Start training\n",
    "    with Logger(args.log_dir) as logger:\n",
    "        for episode in range(args.num_episodes):\n",
    "\n",
    "            if args.algorithm == 'nfsp':\n",
    "                agents[0].sample_episode_policy()\n",
    "\n",
    "            # Generate data from the environment\n",
    "            trajectories, payoffs = env.run(is_training=True)\n",
    "\n",
    "            # Reorganaize the data to be state, action, reward, next_state, done\n",
    "            trajectories = reorganize(trajectories, payoffs)\n",
    "\n",
    "            # Feed transitions into agent memory, and train the agent\n",
    "            # Here, we assume that DQN always plays the first position\n",
    "            # and the other players play randomly (if any)\n",
    "            for ts in trajectories[0]:\n",
    "                agent.feed(ts)\n",
    "\n",
    "            # Evaluate the performance. Play with random agents.\n",
    "            if episode % args.evaluate_every == 0:\n",
    "                logger.log_performance(\n",
    "                    env.timestep,\n",
    "                    tournament(\n",
    "                        env,\n",
    "                        args.num_eval_games,\n",
    "                    )[0]\n",
    "                )\n",
    "\n",
    "        # Get the paths\n",
    "        csv_path, fig_path = logger.csv_path, logger.fig_path\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plot_curve(csv_path, fig_path, args.algorithm)\n",
    "\n",
    "    # Save model\n",
    "    save_path = os.path.join(args.log_dir, 'model.pth')\n",
    "    torch.save(agent, save_path)\n",
    "    print('Model saved in', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Args()\n",
    "args.env = 'leduc-holdem'\n",
    "args.algorithm = 'nfsp' \n",
    "args.cuda = ''\n",
    "args.seed = 42\n",
    "args.num_episodes = 5000\n",
    "args.num_eval_games = 2000\n",
    "args.evaluate_every = 100\n",
    "args.log_dir = 'experiments/leduc_holdem_nfsp/'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "920f746a4452989cea6f2916597b16162ad835700820c23218a0c034e53cffa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
